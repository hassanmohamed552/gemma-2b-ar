{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-12T20:59:34.558881Z",
     "iopub.status.busy": "2024-10-12T20:59:34.558584Z",
     "iopub.status.idle": "2024-10-12T20:59:35.617003Z",
     "shell.execute_reply": "2024-10-12T20:59:35.616015Z",
     "shell.execute_reply.started": "2024-10-12T20:59:34.558848Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T20:59:39.548213Z",
     "iopub.status.busy": "2024-10-12T20:59:39.547103Z",
     "iopub.status.idle": "2024-10-12T20:59:40.414152Z",
     "shell.execute_reply": "2024-10-12T20:59:40.413105Z",
     "shell.execute_reply.started": "2024-10-12T20:59:39.548157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"hf_key\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T20:59:47.878973Z",
     "iopub.status.busy": "2024-10-12T20:59:47.878118Z",
     "iopub.status.idle": "2024-10-12T21:01:32.857509Z",
     "shell.execute_reply": "2024-10-12T21:01:32.856457Z",
     "shell.execute_reply.started": "2024-10-12T20:59:47.878932Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U transformers \n",
    "%pip install -U datasets \n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install -U bitsandbytes \n",
    "%pip install -U mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:01:39.439968Z",
     "iopub.status.busy": "2024-10-12T21:01:39.439554Z",
     "iopub.status.idle": "2024-10-12T21:01:59.599861Z",
     "shell.execute_reply": "2024-10-12T21:01:59.599073Z",
     "shell.execute_reply.started": "2024-10-12T21:01:39.439924Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, mlflow\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:02:05.369048Z",
     "iopub.status.busy": "2024-10-12T21:02:05.367928Z",
     "iopub.status.idle": "2024-10-12T21:02:05.373139Z",
     "shell.execute_reply": "2024-10-12T21:02:05.372258Z",
     "shell.execute_reply.started": "2024-10-12T21:02:05.369005Z"
    }
   },
   "outputs": [],
   "source": [
    "base_model = \"google/gemma-2-2b\"\n",
    "dataset_name = \"premio-ai/TheArabicPile_Articles\"\n",
    "new_model = \"Gemma-2-2b-ar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:02:08.771373Z",
     "iopub.status.busy": "2024-10-12T21:02:08.770971Z",
     "iopub.status.idle": "2024-10-12T21:02:08.835803Z",
     "shell.execute_reply": "2024-10-12T21:02:08.834802Z",
     "shell.execute_reply.started": "2024-10-12T21:02:08.771335Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    !pip install -qqq flash-attn\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:02:17.255443Z",
     "iopub.status.busy": "2024-10-12T21:02:17.255056Z",
     "iopub.status.idle": "2024-10-12T21:02:17.261692Z",
     "shell.execute_reply": "2024-10-12T21:02:17.260736Z",
     "shell.execute_reply.started": "2024-10-12T21:02:17.255406Z"
    }
   },
   "outputs": [],
   "source": [
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:09:30.525573Z",
     "iopub.status.busy": "2024-10-12T21:09:30.525130Z",
     "iopub.status.idle": "2024-10-12T21:09:38.388668Z",
     "shell.execute_reply": "2024-10-12T21:09:38.387724Z",
     "shell.execute_reply.started": "2024-10-12T21:09:30.525531Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397686d59dff4d9aa6e16c8c0a1d63f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:09:55.427550Z",
     "iopub.status.busy": "2024-10-12T21:09:55.427146Z",
     "iopub.status.idle": "2024-10-12T21:09:55.435386Z",
     "shell.execute_reply": "2024-10-12T21:09:55.434484Z",
     "shell.execute_reply.started": "2024-10-12T21:09:55.427512Z"
    }
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:12:58.964717Z",
     "iopub.status.busy": "2024-10-12T21:12:58.964315Z",
     "iopub.status.idle": "2024-10-12T21:12:59.462265Z",
     "shell.execute_reply": "2024-10-12T21:12:59.461231Z",
     "shell.execute_reply.started": "2024-10-12T21:12:58.964679Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:18:59.901440Z",
     "iopub.status.busy": "2024-10-12T21:18:59.901027Z",
     "iopub.status.idle": "2024-10-12T21:19:01.488941Z",
     "shell.execute_reply": "2024-10-12T21:19:01.488054Z",
     "shell.execute_reply.started": "2024-10-12T21:18:59.901397Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 12000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name,\"dedup\", split=\"train\")\n",
    "dataset = dataset.shuffle(seed=65).select(range(12000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:19:17.427570Z",
     "iopub.status.busy": "2024-10-12T21:19:17.427181Z",
     "iopub.status.idle": "2024-10-12T21:19:17.850128Z",
     "shell.execute_reply": "2024-10-12T21:19:17.849351Z",
     "shell.execute_reply.started": "2024-10-12T21:19:17.427535Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T21:19:21.083136Z",
     "iopub.status.busy": "2024-10-12T21:19:21.082484Z",
     "iopub.status.idle": "2024-10-12T22:26:35.115669Z",
     "shell.execute_reply": "2024-10-12T22:26:35.114727Z",
     "shell.execute_reply.started": "2024-10-12T21:19:21.083091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46c511335024f4b9be3f4382d8abcfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214154b967b84b28ac84eb86c208a65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 1:07:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.668700</td>\n",
       "      <td>2.690394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.762800</td>\n",
       "      <td>2.618979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.825200</td>\n",
       "      <td>2.570303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.648300</td>\n",
       "      <td>2.521795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.570800</td>\n",
       "      <td>2.502804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2500, training_loss=2.615820879983902, metrics={'train_runtime': 4025.9427, 'train_samples_per_second': 1.242, 'train_steps_per_second': 0.621, 'total_flos': 9326411910526464.0, 'train_loss': 2.615820879983902, 'epoch': 0.46296296296296297})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Setting Hyperparamter\n",
    "mlflow.set_experiment(\"MLflow PEFT model\")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    report_to=\"mlflow\",\n",
    "    # Name the MLflow run\n",
    "    run_name=f\"gemma-2B-ar-QLoRA-{datetime.now().strftime('%Y-%m-%d-%H-%M-%s')}\",\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    max_steps=2500,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "\n",
    ")\n",
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= 512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T22:27:29.352104Z",
     "iopub.status.busy": "2024-10-12T22:27:29.351705Z",
     "iopub.status.idle": "2024-10-12T22:27:37.546841Z",
     "shell.execute_reply": "2024-10-12T22:27:37.545871Z",
     "shell.execute_reply.started": "2024-10-12T22:27:29.352065Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27794eb4703f44cca4d1269955c43afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/83.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/hassan123mohamed/Gemma-2-2b-ar/commit/6e4063ae344cd1f52f132326064cdc451117cd52', commit_message='Upload model', commit_description='', oid='6e4063ae344cd1f52f132326064cdc451117cd52', pr_url=None, repo_url=RepoUrl('https://huggingface.co/hassan123mohamed/Gemma-2-2b-ar', endpoint='https://huggingface.co', repo_type='model', repo_id='hassan123mohamed/Gemma-2-2b-ar'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.model.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T22:55:47.255021Z",
     "iopub.status.busy": "2024-10-12T22:55:47.254593Z",
     "iopub.status.idle": "2024-10-12T22:55:59.698122Z",
     "shell.execute_reply": "2024-10-12T22:55:59.696864Z",
     "shell.execute_reply.started": "2024-10-12T22:55:47.254982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyngrok\n",
      "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /opt/conda/lib/python3.10/site-packages (from pyngrok) (6.0.2)\n",
      "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: pyngrok\n",
      "Successfully installed pyngrok-7.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T22:46:23.983501Z",
     "iopub.status.busy": "2024-10-12T22:46:23.983093Z",
     "iopub.status.idle": "2024-10-12T22:46:23.987775Z",
     "shell.execute_reply": "2024-10-12T22:46:23.986797Z",
     "shell.execute_reply.started": "2024-10-12T22:46:23.983463Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are a writting assistant to help writing essays and articles in arabic\n",
    "\n",
    "{prompt}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T22:46:29.572752Z",
     "iopub.status.busy": "2024-10-12T22:46:29.572377Z",
     "iopub.status.idle": "2024-10-12T22:46:29.584643Z",
     "shell.execute_reply": "2024-10-12T22:46:29.583704Z",
     "shell.execute_reply.started": "2024-10-12T22:46:29.572717Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [string (required)]\n",
       "outputs: \n",
       "  [string (required)]\n",
       "params: \n",
       "  ['max_new_tokens': long (default: 1024), 'repetition_penalty': double (default: 1.15), 'return_full_text': boolean (default: False)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.models import infer_signature\n",
    "\n",
    "sample = \"اكتب عن الحضارة المصرية\"\n",
    "\n",
    "# MLflow infers schema from the provided sample input/output/params\n",
    "signature = infer_signature(\n",
    "    model_input=sample,\n",
    "    model_output=\"\",\n",
    "    # Parameters are saved with default values if specified\n",
    "    params={\"max_new_tokens\": 1024, \"repetition_penalty\": 1.15, \"return_full_text\": False},\n",
    ")\n",
    "signature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T22:47:01.536593Z",
     "iopub.status.busy": "2024-10-12T22:47:01.535776Z",
     "iopub.status.idle": "2024-10-12T22:47:09.064573Z",
     "shell.execute_reply": "2024-10-12T22:47:09.063761Z",
     "shell.execute_reply.started": "2024-10-12T22:47:01.536550Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 22:47:03 INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.\n",
      "2024/10/12 22:47:04 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained argumentis set to False. The reference to the HuggingFace Hub repository google/gemma-2-2b will be logged instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3f7deb5e3d4b12b065f9de0f934cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/25.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/12 22:47:04 INFO mlflow.transformers: text-generation pipelines saved with prompt templates have the `return_full_text` pipeline kwarg set to False by default. To override this behavior, provide a `model_config` dict with `return_full_text` set to `True` when saving the model.\n",
      "2024/10/12 22:47:04 INFO mlflow.transformers: A local checkpoint path or PEFT model is given as the `transformers_model`. To avoid loading the full model into memory, we don't infer the pip requirement for the model. Instead, we will use the default requirements, but it may not capture all required pip libraries for the model. Consider providing the pip requirements explicitly.\n"
     ]
    }
   ],
   "source": [
    "last_run_id = mlflow.last_active_run().info.run_id\n",
    "# Save a tokenizer without padding because it is only needed for training\n",
    "tokenizer_no_pad = AutoTokenizer.from_pretrained(base_model, add_bos_token=True)\n",
    "\n",
    "# If you interrupt the training, uncomment the following line to stop the MLflow run\n",
    "# mlflow.end_run()\n",
    "\n",
    "with mlflow.start_run(run_id=last_run_id):\n",
    "    mlflow.log_params(peft_config.to_dict())\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": trainer.model, \"tokenizer\": tokenizer_no_pad},\n",
    "        prompt_template=prompt_template,\n",
    "        signature=signature,\n",
    "        artifact_path=\"model\",  # This is a relative path to save model files within MLflow run\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-12T23:02:30.127701Z",
     "iopub.status.busy": "2024-10-12T23:02:30.127252Z",
     "iopub.status.idle": "2024-10-12T23:02:31.330903Z",
     "shell.execute_reply": "2024-10-12T23:02:31.329876Z",
     "shell.execute_reply.started": "2024-10-12T23:02:30.127662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "!ngrok config add-authtoken \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T00:29:08.858393Z",
     "iopub.status.busy": "2024-10-13T00:29:08.857960Z",
     "iopub.status.idle": "2024-10-13T00:29:08.866474Z",
     "shell.execute_reply": "2024-10-13T00:29:08.865414Z",
     "shell.execute_reply.started": "2024-10-13T00:29:08.858350Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-10-13 00:29:11 +0000] [968] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-10-13 00:29:11 +0000] [968] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2024-10-13 00:29:11 +0000] [968] [ERROR] connection to ('127.0.0.1', 5000) failed: [Errno 98] Address already in use\n",
      "[2024-10-13 00:29:12 +0000] [968] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2024-10-13 00:29:12 +0000] [968] [ERROR] connection to ('127.0.0.1', 5000) failed: [Errno 98] Address already in use\n",
      "[2024-10-13 00:29:13 +0000] [968] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2024-10-13 00:29:13 +0000] [968] [ERROR] connection to ('127.0.0.1', 5000) failed: [Errno 98] Address already in use\n",
      "[2024-10-13 00:29:14 +0000] [968] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2024-10-13 00:29:14 +0000] [968] [ERROR] connection to ('127.0.0.1', 5000) failed: [Errno 98] Address already in use\n",
      "[2024-10-13 00:29:15 +0000] [968] [ERROR] Connection in use: ('127.0.0.1', 5000)\n",
      "[2024-10-13 00:29:15 +0000] [968] [ERROR] connection to ('127.0.0.1', 5000) failed: [Errno 98] Address already in use\n",
      "[2024-10-13 00:29:16 +0000] [968] [ERROR] Can't connect to ('127.0.0.1', 5000)\n",
      "Running the mlflow server failed. Please see the logs above for details.\n"
     ]
    }
   ],
   "source": [
    "get_ipython().system_raw(\"mlflow ui --port 5000 &\") # run tracking UI in the background\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T00:30:06.869886Z",
     "iopub.status.busy": "2024-10-13T00:30:06.869015Z",
     "iopub.status.idle": "2024-10-13T00:30:06.878254Z",
     "shell.execute_reply": "2024-10-13T00:30:06.877239Z",
     "shell.execute_reply.started": "2024-10-13T00:30:06.869843Z"
    }
   },
   "outputs": [],
   "source": [
    "ngrok.kill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T00:28:17.332345Z",
     "iopub.status.busy": "2024-10-13T00:28:17.331656Z",
     "iopub.status.idle": "2024-10-13T00:28:29.052892Z",
     "shell.execute_reply": "2024-10-13T00:28:29.051722Z",
     "shell.execute_reply.started": "2024-10-13T00:28:17.332304Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyngrok --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T00:30:46.043079Z",
     "iopub.status.busy": "2024-10-13T00:30:46.042704Z",
     "iopub.status.idle": "2024-10-13T00:30:46.538420Z",
     "shell.execute_reply": "2024-10-13T00:30:46.537513Z",
     "shell.execute_reply.started": "2024-10-13T00:30:46.043043Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow Tracking UI: https://6c03-34-91-255-126.ngrok-free.app\n"
     ]
    }
   ],
   "source": [
    "ngrok_tunnel = ngrok.connect(addr=\"5000\", proto=\"http\", bind_tls=True)\n",
    "print(\"MLflow Tracking UI:\", ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T00:51:01.903143Z",
     "iopub.status.busy": "2024-10-13T00:51:01.902734Z",
     "iopub.status.idle": "2024-10-13T00:51:13.716021Z",
     "shell.execute_reply": "2024-10-13T00:51:13.714796Z",
     "shell.execute_reply.started": "2024-10-13T00:51:01.903107Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install -U accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T01:32:54.261006Z",
     "iopub.status.busy": "2024-10-13T01:32:54.260620Z",
     "iopub.status.idle": "2024-10-13T01:33:02.341692Z",
     "shell.execute_reply": "2024-10-13T01:33:02.340887Z",
     "shell.execute_reply.started": "2024-10-13T01:32:54.260971Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/13 01:32:54 WARNING mlflow.transformers.model_io: Could not specify device parameter for this pipeline type.Falling back to loading the model with the default device.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0137504fd90e4a988fd9231937e6749f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow_model = mlflow.pyfunc.load_model(\"runs:/5dfb70f457b54fff8dcc10f2b45a5668/model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T01:44:11.952610Z",
     "iopub.status.busy": "2024-10-13T01:44:11.952205Z",
     "iopub.status.idle": "2024-10-13T01:44:21.579582Z",
     "shell.execute_reply": "2024-10-13T01:44:21.578650Z",
     "shell.execute_reply.started": "2024-10-13T01:44:11.952574Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/13 01:44:11 WARNING mlflow.transformers: params provided to the `predict` method will override the inference configuration saved with the model. If the params provided are not valid for the pipeline, MlflowException will be raised.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ancient Egyptian civilization was one of the most important civilizations that existed during the first millennium BC. It is considered as an example for all civilized societies, because it has left behind many monuments such as pyramids, temples and statues. The Egyptians have been able to build these great works thanks to their advanced technology at that time which allowed them to use stone tools made from granite or limestone with precision so much so that some people believe there may be more than 10 different types of stones used by Ancient Egypt!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_table(dataset_or_sample):\n",
    "    # A helper fuction to display a Transformer dataset or single sample contains multi-line string nicely\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    pd.set_option(\"display.width\", None)\n",
    "    pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "\n",
    "test_prompt = \"\"\"\n",
    "### title:\n",
    "الحضارة المصرية القديمة\n",
    "\n",
    "### Question:\n",
    "write about pharaohs and how they were connected other civilisation in arabic\n",
    "\"\"\"\n",
    "\n",
    "generated_query = mlflow_model.predict(test_prompt)[0]\n",
    "print(generated_query)\n",
    "display_table({\"prompt\": test_prompt, \"generated_query\": generated_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T02:10:04.015807Z",
     "iopub.status.busy": "2024-10-13T02:10:04.015430Z",
     "iopub.status.idle": "2024-10-13T02:10:38.593394Z",
     "shell.execute_reply": "2024-10-13T02:10:38.592398Z",
     "shell.execute_reply.started": "2024-10-13T02:10:04.015776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e1c8371c0e4b5daeb75371fc1eeeff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "اكتب عن الحضارة المصرية القديمة المقال : 1 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 2 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 3 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 4 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 5 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 6 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 7 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 8 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 9 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 10 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 11 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 12 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 13 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 14 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 15 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 16 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 17 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل الميلاد. 18 - الحضارة المصرية القديمة هي الحضارة التي نشأت في مصر منذ 5000 سنة قبل\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"hassan123mohamed/Gemma-2-2b-ar-final\",\n",
    "               device=\"cuda\",)\n",
    "text = \"اكتب عن الحضارة المصرية القديمة\"\n",
    "outputs = pipe(text, max_new_tokens=512)\n",
    "response = outputs[0][\"generated_text\"]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T02:12:47.942049Z",
     "iopub.status.busy": "2024-10-13T02:12:47.941645Z",
     "iopub.status.idle": "2024-10-13T02:13:05.137825Z",
     "shell.execute_reply": "2024-10-13T02:13:05.136537Z",
     "shell.execute_reply.started": "2024-10-13T02:12:47.942013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-5.0.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.4.0)\n",
      "Requirement already satisfied: fastapi<1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.111.0)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting gradio-client==1.4.0 (from gradio)\n",
      "  Downloading gradio_client-1.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.10.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (10.3.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.9.2)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.0.9)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.2)\n",
      "Collecting ruff>=0.2.2 (from gradio)\n",
      "  Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomlkit==0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.12.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.30.1)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.4.0->gradio) (2024.6.1)\n",
      "Requirement already satisfied: websockets<13.0,>=10.0 in /opt/conda/lib/python3.10/site-packages (from gradio-client==1.4.0->gradio) (12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio) (1.2.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.37.2)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.2 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (0.0.4)\n",
      "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (5.10.0)\n",
      "Requirement already satisfied: email_validator>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from fastapi<1.0->gradio) (2.1.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (3.15.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.25.1->gradio) (4.66.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->gradio) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi<1.0->gradio) (2.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.12.0->fastapi<1.0->gradio) (0.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.25.1->gradio) (1.26.18)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Downloading gradio-5.0.2-py3-none-any.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading gradio_client-1.4.0-py3-none-any.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
      "Downloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
      "Installing collected packages: tomlkit, semantic-version, ruff, ffmpy, gradio-client, gradio\n",
      "  Attempting uninstall: tomlkit\n",
      "    Found existing installation: tomlkit 0.13.2\n",
      "    Uninstalling tomlkit-0.13.2:\n",
      "      Successfully uninstalled tomlkit-0.13.2\n",
      "Successfully installed ffmpy-0.4.0 gradio-5.0.2 gradio-client-1.4.0 ruff-0.6.9 semantic-version-2.10.0 tomlkit-0.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-13T02:14:44.073123Z",
     "iopub.status.busy": "2024-10-13T02:14:44.072675Z",
     "iopub.status.idle": "2024-10-13T02:14:50.122903Z",
     "shell.execute_reply": "2024-10-13T02:14:50.121982Z",
     "shell.execute_reply.started": "2024-10-13T02:14:44.073079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "Kaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "* Running on public URL: https://754c934ce9c9fd0201.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://754c934ce9c9fd0201.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(prompt):\n",
    "    completion = model(prompt)[0][\"generated_text\"]\n",
    "    return completion\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "gr.Interface(fn=predict, inputs=\"text\", outputs=\"text\").launch()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
